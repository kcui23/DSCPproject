<!DOCTYPE html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Project proposal</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        .external-link::after {
            content: "\f35d";
            font-family: 'Font Awesome 5 Free';
            font-weight: 900;
            margin-left: 5px;
            font-size: 0.8em;
        }
        .members {
            font-size: 1.25em;
            margin-top: 5px;
            font-weight: normal;
            color: #555;
        }
        p {
            font-size: 1.1em;
            margin-top: 5px;
            font-weight: normal;
        }
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css">
</head>
<body>
    <div class="container">
        <br>
        <h1>Project proposal</h1>
        <div class="members">
            Project Group 8: Baiheng Chen, Kai Cui, Tuocheng Chen, Yuda Ding, Ziyi Song
        </div>
        <br>
        <h3>Code to get data</h3>
        <p>Here is the code to get data from the Gutenberg ebooks project. We use the <code>wget</code> command to download the data. The code is written in bash and is run on the cluster.</p>
        <h5>- submit.sh</h5>
        <pre><code class="language-bash">#!/bin/bash

rm -rf slurm_out;mkdir -p slurm_out
rm -rf data;mkdir -p data

sbatch --output="slurm_out/slurm-%A_%a.out" \
        --error="slurm_out/slurm-%A_%a.err" \
        --array 1-3000 getData.sh</code></pre>
        <br>
        <h5>- getData.sh</h5>
        <pre><code class="language-bash">#!/bin/bash

base_url="http://aleph.gutenberg.org"
n=$SLURM_ARRAY_TASK_ID
start=$(echo "20 * ($n - 1) + 10001" | bc)
end=$(echo "20 * $n + 10000" | bc)

download_file() {
    dir1=$(printf "%d" $(($1/10000)))
    dir2=$(printf "%d" $(($1%10000/1000)))
    dir3=$(printf "%d" $(($1%1000/100)))
    dir4=$(printf "%d" $(($1%100/10)))

    for suffix in "" "-0" "-8"; do
        url="${base_url}/${dir1}/${dir2}/${dir3}/${dir4}/${1}/${1}${suffix}.txt"
        wget --spider $url

        if [ $? -eq 0 ]; then
            wget -P data $url
        break
        fi
    done
}
for i in $(seq $start $end);do
    download_file $i
done</code></pre>
        <br>
        <h3>About our dataset</h3>
        <h4>Dataset Overview</h4>
        <p>We are using text data from e-books, sourced from http://aleph.gutenberg.org. We have extracted about 60,000 e-books for certain statistical analyses. We used 3000 jobs for parallel downloading, ultimately obtaining 59,180 TXT files, totaling 22GB.It would be an intersting and challenging work to implement some valuable analysis with HPC on such a huge dataset.</p>
        <h4>Descriptions of the variables</h4>
        <p>All we have are text variables. Specifically, we have a fixed format head of each ebook including some basic information, here's an example:</p>
        <br>
        Title: Apocolocyntosis
        <br>
        Author: Lucius Seneca
        <br>
        Release Date: November 10, 2003 [EBook #10001]
        <br>
        [Date last updated: April 9, 2005]
        <br>
        Language: English
        <br>
        Character set encoding: ASCII
        <br>
        <p>And the whole books will go after the heads.</p>
        <h4>Computational Steps for Each Method</h4>
        <h5>Text Style Analysis for Book Recommendation</h5>
        <p>Extract stylistic features such as sentence length, vocabulary diversity, use of adj/adv, etc.
Then apply clustering algorithms like K-means to group books with similar styles.
We will parallelize by book in the feature extraction part and cluster with aggregated data.</p>
        <h5>Popularity of Styles Over Time</h5>
        <p>Organize the books by their publication date.
Apply the style analysis from the starting point.
Use time series analysis to detect trends.
We will parallelize by decade or other time divisions suitable, analyzing each period in parallel.</p>
        <h5>Style Change Across Time for Specific Authors</h5>
        <p>Filter the books by the specific author. 
Then apply text style analysis for each book.
Use regression analysis to track changes in style over time.
The parallelization will be by book, with an extra step to order the results by time.</p>
        <h5>Difficulty Level of Reading Different Books</h5>
        <p>Calculate readability scores (such as the Flesch-Kincaid Grade Level or Gunning Fog Index) which consider sentence length and word difficulty.
Consider different levels of difficulty level.
We will parallelize this by computing the scores for multiple books at once, then aggregate the data.</p>
        <h5>Emotional Tendencies of Different Books</h5>
        <p>Count the frequency of words associated with different emotions.
Construct emational analysis of each book. 
Apply clustering algorithms.
Like in 1, the parallelization happens by book, with the results aggregated for later clustering.</p>
        <h3>Link to the Github repository</h3>
        <p>You can find our project here: <a href="https://github.com/kcui23/stat605_final_project/" target="_blank" class="external-link">https://github.com/kcui23/stat605_final_project/</a></p>
        <br>
        <br>
        <br>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/components/prism-bash.min.js"></script>
</body>
</html>
